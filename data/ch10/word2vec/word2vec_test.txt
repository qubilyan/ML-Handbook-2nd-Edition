Chapter 1

What Is Machine Learning?

Let’s start at the start, looking at what machine learning actually is, it’s history and where it is used in industry. I’ll also run down some of the software used throughout the book too so you can have everything installed and be ready to get working on the practical things.

The Definition Of Machine Learning

So what is the definition of machine learning? Over the last six decades we’ve had the pioneers steering us in the right direction.

Alan Turing

Alan Turing posed the question, “Can machines think?” in his 1950 paper “Computing Machinery and Intelligence”. The paper went on to suggest the “Imitation Game” where three participants, a human acting as a judge, another human and a computer which is attempting to convince the judge that it is human. The judge would type into a terminal program and “talk” to the other two participants. Both the human and the computer respond and the judge decide which is the computer. During the test if the judge consistently can’t tell the difference between the human and computer responses then the computer has won the game.

The test goes on today in the form of the Loebner Prize which is an annual competition in artificial intelligence. The aim is simple enough; convince the judges that they are chatting to a human instead of a computer chat bot program.

Arthur Samuel

“[A] Field of study that gives computers the ability to learn without being explicitly programmed” was the definition of machine learning that Arthur Samuel gave back in 1959. Samuel is credited to have one of the self-learning computer programs with his work at IBM, he focused on games as a way of getting the computer to learn things.

The game of choice for Samuel was checkers as it was a simple game but with enough strategy involved that the program could learn from. With the use of alpha-beta evaluation pruning and minimax strategies the program would discount moves and thus improve costly memory performance of the program.

While Samuel is widely know for his work in artificial intelligence he was also noted for being one of the first programmers to use hash tables, he certainly made a big impact at IBM.

Tom M. Mitchell

Tom M. Mitchell is the Chair of Machine Learning at Carnegie Mellon University. As author of the book “Machine Learning” he is much quoted with the definition:

“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves the experience E.”

The important thing here is that we now have a set of objects to define our machine learning.

• Task (T), either one or more.
• Experience (E)
• Performance (P)
So with a computer running a set of tasks the experience should be leading to performance increases.

To Sum Up

Machine learning is a branch of artificial intelligence. Using computing we design systems that can learn from data in either a manner of being trained or not. The systems may learn and improve with experience and with time refine a model that can be used to predict outcomes of questions based on the previous learning.



Algorithmic Types For Machine Learning

There are a number of different methods that can be employed to machine learning. The deciding factor on what algorithm to use is really based on the required output. As you work through the chapters you’ll see the different algorithm types being put to work.

Supervised Learning

When we talk about supervised learning we are working a set of labeled training data. For every example in the training data we have an input object and an output object. An example would be classifying Twitter data (I’ll be using Twitter data lot in the final two chapters) lets assume we have the following data from Twitter, this would be our input data objects.

Really loving the new St Vincent album!
#fashion I’m selling my Louboutins! Who’s interested? #louboutins
I’ve got my Hadoop cluster working on a load of data. #data
In order for our supervised learning classifier to know the outcome result of each tweet we have to manually put the answers in, for clarity I’ve added the resulting output object at the start of the line.

music​​Really loving the new St Vincent album!
clothing​#fashion I’m selling my Louboutins! Who’s interested? #louboutins
bigdata​​I’ve got my Hadoop cluster working on a load of data. #data
Obviously for the classifier to make any sense of the data when run properly we’ll have to manually work on a lot more input data. What we have though is a training set that can be used for later classification of data.

There are issues with supervised learning that have to be taken in to account. The bias-variance dilemma is one of them, how the machine-learning model performs accurately against different training sets. With high bias models with restricted learning or high variance, which learn with complexity against noisy training data. There’s a trade off between the two. The key is where to settle with the trade off.

Unsupervised Learning

On the opposite side of the coin, unsupervised learning is where we let the algorithm just find a hidden pattern in a load of data. There’s no error in the solution we propose it’s just a case of running the machine-learning algorithm and seeing what comes out.

The mention of unsupervised learning may be more a case of just data mining. If you’re looking at clustering data then there’s a good chance you’re going to spend a lot of time with unsupervised learning.

The Human Touch

Outcomes will change, data will change and requirements will change. Machine learning cannot be seen as the write it once solution to problems. Also it will require human hands to and intuition to write these algorithms to start off with. Remember that Arthur Samuel’s checkers program was basically to improve on what the human had already taught it. It all started off with a human first and it’s important that we get this message across.

During the course of this book you will see me talk about the importance of knowing what the question is we are trying to answer, the cornerstone of any data project, this starts with us having open discussions and planning (more of this in chapter 2).

It’s only in very rare circumstances that you can throw data at a machine learning routine and it start to provide insight.

Uses For Machine Learning

So what can we use machine learning for? Quite a lot really, when you break things down and see how it’s being used at the moment.

Software

Spam Detection

“I don’t like Spam!” said the Monty Python team. For all the junk mail that gets caught there’s a good chance there’s a Bayesian classification filtering doing the work to catch it. Since the early days of SpamAssassin to Google’s work in Google Mail there’s been some form of learning attempting to figure out whether a message is good or bad.

Spam detection is one of the classic uses of machine learning and over time the algorithms have got better and better. Think about the email program that you use when it sees a message it thinks is junk it will ask you to confirm whether it is or isn’t. If you decide it is the system will learn from that message and learn from the experience. Future messages will, hopefully, be treated correctly from then on.

Voice Recognition

Apple’s Siri service that is on many iOS devices is another example of software machine learning. You ask Siri something and it works out what you want to do. This may be sending a tweet or a text message or it could be setting a calendar appointment, if Siri decides it can’t work out what you’re trying to do it will perform a Google search on the phrase you said.

It’s an impressive service that is using a device and cloud based statistical model to analyze your phrase, the order of the words and come up with a resulting action to do on the device.

Trading

There are lots of trading platforms aiming to help users to do better stock trades. This comes with a large amount of analysis, computation and recommendation. From a machine learning perspective decisions are being made to you on whether to buy or sell a stock based at the current price. It takes in to account the historical opening and closing prices and the buy sell volumes of that stock.

With four pieces of information (the low and high price plus the opening and closing price of the day) a machine learning algorithm can learn. Apply this will all stocks and you have a system.

Bitcoins are a good example of trading; the virtual coins are bought and sold on what price the market is willing to buy against what existing coin owners are willing to sell at.

The press is much more interested in algorithmic trading especially of the high-speed variety. The ability to perform many thousands of trades a second based on algorithmic prediction is very compelling. A huge amount of money is ploughed into these systems and how close they are to the main stock trading exchanges. Milliseconds of network latency can cost the trading house millions in trades that weren’t placed in time.

About 70% of the trades done are performed by machine and not on the trading floor. This is all very well when things are going fine but when a problem occurs it can be minutes before the fault is noticed, by which time so many trades have happened. The flash crash in May 2010 is a good example of this when the Dow Jones industrial average dived by 600 points.

Robotics

Using machine learning robots can acquire skills or learn to adapt to the environment it is working in. Activities such as object placement, grasping objects and locomotion skills are achieved with either automated learning or learning via human intervention.

With the increasing amount of sensors within robotics other algorithms could be employed outside of the robot for further analysis.

Medicine And Healthcare

The race is on for machine learning to be used in healthcare analytics. A number of startups are looking at the advantages of using machine learning with Big Data to provide healthcare professionals better-informed data to enable them to give better decisions.

IBM’s famed Watson supercomputer, once used to win the television quiz programme Jeopardy against two human contestants, is being used to help doctors. Using Watson as a service on the cloud doctors can access learning on millions of pages of medical research and hundreds of thousands of pieces of information on medical evidence.

With the number of consumers using smartphones and the related devices for collating a range of health information such as weight, heart rate, pulse, pedometers, blood pressure and even blood glucose monitoring we can now track and trace user health regularly and see patterns in dates and times. Machine learning systems can recommend healthier alternatives back to the user via the device.

While it’s easy enough to analyze data the privacy of user health data is another story. Obviously some users are more concerned about how their data is used especially in the case of it being sold on to third party companies. Analytics in healthcare and medicine is new at the volumes that are now available but the privacy debate will be the ultimate deciding factor about how the algorithms will be used.



Advertising

Since the dawn of time there’s always been a brand trying to influence us to buy their produce. Post 1995 and the Internet gave marketers the chance to advertise direct to our screens without the need of television or large print campaigns. Remember the thought of cookies being on our computers with the potential to track us? The race to disable cookies from browsers and control whom saw our habits was big news at the time.

Log file analysis is another tactic that advertisers use to see the things we are interested in. They are able to cluster results and segment user groups who may be interested in specific types of products. Couple that with mobile location awareness and you have direct-targeted adverts sent direct to you.

There was a time this was considered a huge invasion of privacy but over time we’ve gotten use to the idea, now we’re even happy to check in at a location and announce our arrival. All very well but don’t think no one is watching, far from it, plenty will be learning from it. With some learning and analysis advertisers can do a very good job in figuring out where you’ll be on a given day and attempt to push offers your way.

Retail And E-Commerce

Machine learning is heavily used in retail, both e-commerce and bricks and mortar retail. At the high level the obvious use case is the loyalty card, you may be in possession of some. Retailers that issue loyalty cards often struggle to make sense of the data that’s coming back to them. Having worked with one company that analyzed this data I know the pain that supermarkets go through to get insight.

In the UK supermarket giant Tesco are the name in the UK when it comes to a customer loyalty programme. The Tesco Clubcard is used heavily by customers and gives Tesco a great view of customer purchasing decisions. Data is collected from the point of sale (POS) and fed back to a data warehouse. In the early stages of the Clubcard the data couldn’t be mined fast enough, there was just too much. As processing methods improved over the years Tesco have a very good strategy along with marketing company Dunn Humby about customer behavior, shopping habits and encouraging customers to try similar products away from their usual choices.

For the American equivalent we have to look at the likes of Target who run a similar sort of program that tracks every customer engagement with the brand. This might be via mail outs, web site visits or even in-store visits. From the data warehouse Target can fine-tune how to get the right communication method to the right customer in order for them to react to the brand. Target learned that not every customer wants an email or a SMS message, some still prefer receiving mail via the postal service.

The uses for machine learning in retail are obvious, mining baskets and segmenting users are key processes in order to communicate the right message to the customer. On the other hand it can too accurate and cause headaches. Target’s baby club story, that was widely cited in the press as a big privacy danger in Big Data, showed us that machine learning basically can easily know that we’re creatures of habit and when those habits change they will get noticed.

For all the positive uses of machine learning there are some urban myths too. Talk of retail and data mining the “beer and diapers” story is often mentioned and associated with Walmart and other large retailers. The idea was that on a Friday the sales of beer and diapers would right together suggesting that the child’s mother was going out and dad would stock up on beer for while he was at home and diapers for the one he was looking after. It turned out to be a myth, thought this still doesn’t stop marketing companies wheeling out the story (and still believing it’s true) to organizations who want to learn from their data.

Likewise the heavy metal band Iron Maiden did not mine bit-torrent data to figure out which countries were illegally downloading their songs and then fly to play to fans. Another myth that got the marketers and media very exited about Big Data and machine learning, but sadly untrue. Not to say that these can’t happen, they just haven’t happened yet.

Gaming Analytics

We’ve already established that checkers is a good candidate for machine learning. Do you remember those old chess computer games with the real plastic pieces? You made a move and the computer made a move. Well that’s machine learning planning algorithms in action. Fast-forward a few decades (the chess computer still feels like yesterday to me) and the console market is pumping out analytics data every time you play your favorite game.

Microsoft have spent time studying the data from Halo 3 to see how players perform on certain levels but also figure out when a player is using a cheat. Fixes have been created based on the analysis of data coming back from the consoles.

The same company also worked on Drivatar that is incorporated in to the driving game Forza Motorsport. When the game is first played it knows nothing about your driving style. Over a period of practice laps the system learns your style, consistency, exit speeds on corners and your positioning on the track. The sampling happens over three laps which is enough time to see how your profile behaves. As time progresses the system continues to learn from your driving patterns.

If you have children you may have seen the likes of Ninendogs (or cats) where it’s your task to look after an on-screen pet. Think a Tamagotchi but on a larger scale. Algorithms can work out when the pet needs play, how to react to the owner and how hungry the pet will be.

It’s early days for games companies to get machine learning into infrastructure to make the games better. With more and more games appearing on small devices such as the iPhone and Android platforms the real learning is on how to make players come back and play more and more. Analysis can be performed about the “stickiness” of the game, do players return to play again or do they drop off over a period of time in favor of something else. Ultimately there’s a trade off between the level of machine learning and gaming performance especially in smaller devices.

The Internet Of Things

Connected devices that can collate a